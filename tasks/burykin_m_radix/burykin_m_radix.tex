\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{tocloft}
\usepackage{xcolor} 
\usepackage{listings}
\usepackage{float}

\geometry{a4paper, left=25mm, right=15mm, top=20mm, bottom=20mm}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=black,
    pdftitle={Отчет по проекту},
    pdfauthor={Бурыкин Михаил},
    pdfsubject={Поразрядная сортировка для целых чисел с простым слиянием.}
}

\lstset{
  language=C++,
  basicstyle=\ttfamily\small,
  keywordstyle=\bfseries\color{blue},
  commentstyle=\itshape\color{green!50!black},
  stringstyle=\color{red!60!black},
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=8pt,
  backgroundcolor=\color{gray!10},
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  frame=single,
  tabsize=2,
  captionpos=b
}

\renewcommand{\cftsecaftersnum}{.}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}

\begin{document}
\begin{titlepage}
    \centering
    \large
    Министерство науки и высшего образования Российской Федерации\\[0.5cm]
    Федеральное государственное автономное образовательное учреждение высшего образования\\[0.5cm]
    \textbf{«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»}\\
    (ННГУ)\\[1cm]
    Институт информационных технологий, математики и механики\\[0.5cm]
    Направление подготовки: \textbf{«Программная инженерия»}\\[2cm]

    \vfill
    {\LARGE \textbf{ОТЧЕТ}}\\[0.5cm]
    {\Large по задаче}\\[0.5cm]
    {\LARGE \textbf{«Поразрядная сортировка для целых чисел с простым слиянием»}}\\[0.5cm]
    {\Large \textbf{Вариант №17}}\\[2.5cm]

    \hfill\parbox{0.5\textwidth}{
        \textbf{Выполнил:} \\
        студент группы 3822Б1ПР3 \\
        \textbf{Бурыкин Михаил}
    }\\[0.5cm]

    \hfill\parbox{0.5\textwidth}{
        \textbf{Преподаватели:} \\
        Нестеров А.Ю.
        Оболенский А.А.

    }\\[2cm]

    Нижний Новгород\\
    2025
\end{titlepage}


\thispagestyle{empty}
\clearpage
\pagenumbering{arabic} 
\setcounter{page}{2} 
\tableofcontents
\clearpage
\setcounter{page}{3} 
\section{Введение}

\hspace*{1.25em}Сортировка массивов - одна из ключевых задач в программировании, которая лежит в основе множества прикладных и научных приложений: от обработки данных до алгоритмов машинного обучения и информационного поиска. Эффективные алгоритмы сортировки способны значительно сократить время выполнения программ и повысить их производительность, что особенно важно при работе с большими объемами данных.

Алгоритм поразрядной сортировки (Radix Sort), впервые описанный в 1887 году, представляет собой не сравнительный метод сортировки, который обрабатывает элементы по разрядам — обычно по байтам. Его суть заключается в последовательной сортировке массива по каждому разряду, начиная с младшего, что делает его особенно эффективным для данных с фиксированным размером, таких как целые числа. 

В данной работе рассматривается модифицированная версия Radix Sort, включающая этап простого слияния отсортированных блоков. Такой подход упрощает структуру кода и создает дополнительные возможности для параллельных вычислений, что особенно актуально в современных условиях.

Сегодняшние вычислительные системы предлагают широкий спектр технологий для распараллеливания задач: от многоядерных процессоров до кластеров и гибридных архитектур. В этом отчете исследуются возможности параллельной реализации Radix Sort с использованием последовательного подхода (SEQ), а также технологий OpenMP (OMP), Intel Threading Building Blocks (TBB), стандартной библиотеки потоков C++ (STL) и комбинации MPI с OMP (MPI+OMP).

Цель работы — провести сравнительный анализ производительности этих подходов и определить наиболее эффективные стратегии для обработки больших массивов данных в многопоточной среде.

\section{Постановка задачи}

\hspace*{1.25em}Целью данной работы является разработка и исследование различных реализаций алгоритма поразрядной сортировки с последующим простым слиянием отсортированных блоков.

Для достижения этой цели необходимо решить следующие задачи:

\begin{itemize}
\item Реализовать последовательную версию алгоритма Radix Sort (SEQ) с учетом поэтапного слияния;
\item Разработать параллельные версии алгоритма с использованием следующих технологий:
\begin{itemize}
\item OpenMP (OMP) — стандарт для параллельного программирования на системах с общей памятью;
\item Intel TBB — библиотека для потокового параллелизма с автоматическим управлением задачами;
\item std::thread (STL) — инструменты стандартной библиотеки C++ для ручного управления потоками;
\item MPI + OMP — гибридный подход, объединяющий распределенные вычисления и многопоточность.
\end{itemize}
\item Провести тестирование и проверку корректности всех реализованных версий алгоритма;
\item Оценить производительность последовательной и параллельных реализаций по времени выполнения и масштабируемости при увеличении объема данных.
\end{itemize}

Ожидаемым результатом работы является определение преимуществ и ограничений каждого из подходов к параллельной реализации Radix Sort, а также выбор оптимальной технологии для обработки больших массивов целых чисел.

\section{Описание алгоритма}

Алгоритм поразрядной сортировки (Radix Sort) представляет собой не сравнительный метод сортировки, который обрабатывает элементы массива по их разрядам, обычно по байтам. Основная идея заключается в последовательной сортировке массива по каждому разряду, начиная с младшего, что позволяет эффективно упорядочивать данные с фиксированным размером, такие как целые числа.

Принцип работы алгоритма можно описать следующим образом:
\begin{enumerate}
\item Для каждого байта (8-битного разряда) в целочисленных элементах массива выполняется подсчет количества элементов с одинаковыми значениями в текущем разряде.
\item На основе подсчитанных частот вычисляются начальные индексы для размещения элементов в выходном массиве.
\item Элементы распределяются в выходной массив в соответствии с их значением в текущем разряде, сохраняя относительный порядок элементов с одинаковыми значениями в разряде.
\item Процесс повторяется для каждого байта, начиная с младшего (наименее значимого) и заканчивая старшим (наиболее значимым).
\end{enumerate}

В данной работе к базовому алгоритму поразрядной сортировки добавляется этап поэтапного слияния отсортированных блоков:
\begin{itemize}
\item После того как каждый поток (или процесс) завершает сортировку своей части массива, происходит попарное объединение отсортированных блоков.
\item Для слияния используется стандартный алгоритм merge с временным буфером, что обеспечивает стабильность и корректность результата.
\item Этап слияния продолжается до тех пор, пока не останется один общий отсортированный массив.
\end{itemize}

\section{Описание параллельных реализаций алгоритма}

\subsection{OpenMP}

Параллельная реализация поразрядной сортировки (Radix Sort) с использованием технологии OpenMP выполнена в классе \texttt{RadixOMP}, определённом в пространстве имён \texttt{burykin\_m\_radix\_omp}. Основной целью данной реализации является распараллеливание ключевых этапов алгоритма: вычисления частот, расчёта индексов и распределения элементов по выходному массиву.

\textbf{Структура реализации:}

\begin{enumerate}
  \item \textbf{Вычисление частот (\texttt{ComputeFrequency}).} 
  Для каждого байта входного массива вычисляется частота встречаемости каждого возможного значения (0--255). Этот этап выполняется параллельно с использованием OpenMP:
  \begin{itemize}
    \item Каждый поток поддерживает локальный счётчик \texttt{local\_count} для избежания конфликтов при обновлении общего счётчика.
    \item Директива \texttt{\#pragma omp parallel} создаёт параллельный регион, а \texttt{\#pragma omp for nowait schedule(static)} распределяет итерации цикла по потокам с использованием статического планирования без ожидания их завершения.
    \item Локальные счётчики объединяются в общий массив \texttt{count} с использованием атомарных операций \texttt{\#pragma omp atomic}, что минимизирует накладные расходы на синхронизацию по сравнению с критическими секциями.
    \item Для старшего байта (сдвиг на 24 бита) применяется преобразование ключа с помощью операции \verb|key ^= 0x80| для корректной обработки знакового бита.
  \end{itemize}

  \item \textbf{Расчёт индексов (\texttt{ComputeIndices}).}
  На основе частот вычисляются начальные индексы для размещения элементов в выходном массиве. Этот этап остаётся последовательным из-за зависимости данных: каждый следующий индекс зависит от суммы предыдущих частот. Результат сохраняется в массиве \texttt{index}.

  \item \textbf{Распределение элементов (\texttt{DistributeElements}).}
  Элементы распределяются в выходной массив на основе значений текущего разряда. Для эффективной параллельной обработки:
  \begin{itemize}
    \item Каждый поток получает свою копию массива индексов \texttt{thread\_indices}, инициализированную общими индексами.
    \item Входной массив делится на части, и каждый поток обрабатывает свой сегмент, подсчитывая локальные частоты в \texttt{thread\_counts}.
    \item Локальные индексы корректируются на основе глобальных смещений, вычисленных из \texttt{thread\_counts}, что исключает необходимость использования критических секций.
    \item Элементы копируются в выходной массив \texttt{b} с использованием локальных индексов, обновляемых каждым потоком независимо.
    \item Директива \texttt{\#pragma omp parallel} управляет параллельным выполнением, а данные распределяются между потоками с учётом числа доступных потоков (\texttt{omp\_get\_max\_threads()}).
  \end{itemize}

  \item \textbf{Общая организация выполнения.}
  Основной цикл сортировки, реализованный в методе \texttt{RunImpl}, последовательно проходит по разрядам (сдвиги на 0, 8, 16, 24 бита). Для каждого разряда вызываются \texttt{ComputeFrequency}, \texttt{ComputeIndices} и \texttt{DistributeElements}. Входной и выходной массивы чередуются с помощью \texttt{swap}, а результат сохраняется в \texttt{output\_}.
  
  \item \textbf{Постобработка (\texttt{PostProcessingImpl}).}
  После завершения сортировки элементы из внутреннего массива \texttt{output\_} копируются в выходной буфер задачи. Копирование выполняется параллельно с использованием директивы \texttt{\#pragma omp parallel for schedule(static, 1024)}, где размер блока (1024) оптимизирует распределение работы между потоками.
\end{enumerate}

\textbf{Преимущества реализации:}
\begin{itemize}
  \item Использование атомарных операций вместо критических секций в \texttt{ComputeFrequency} снижает накладные расходы на синхронизацию.
  \item Применение пер-поточных массивов индексов в \texttt{DistributeElements} устраняет необходимость в критических секциях, повышая масштабируемость.
  \item Статическое планирование и оптимизированный размер блоков в параллельных циклах обеспечивают равномерное распределение нагрузки.
  \item Эффективная работа на многоядерных процессорах, особенно для больших массивов, благодаря параллелизации вычислений и минимизации конфликтов.
\end{itemize}

\textbf{Пример:}
При сортировке массива из 10000 элементов с использованием 4 потоков каждый поток обрабатывает примерно 2500 элементов на этапе вычисления частот. На этапе распределения элементы записываются в выходной массив с использованием локальных индексов, что исключает гонки данных и ускоряет выполнение за счёт одновременной работы нескольких ядер.

\subsection{Intel TBB}

Реализация поразрядной сортировки (Radix Sort) с использованием библиотеки Intel oneAPI Threading Building Blocks (TBB) основана на модели task-based параллелизма, которая эффективно использует ресурсы многоядерных процессоров. Параллелизм применяется на этапах вычисления частот и распределения элементов, что значительно ускоряет выполнение алгоритма.

\textbf{Ключевая функция реализации:} \texttt{RadixTBB::RunImpl}.

\textbf{Основные этапы:}
\begin{enumerate}
  \item \textbf{Инициализация:}
  \begin{itemize}
    \item Размер входного массива: $n = \text{input\_}.size()$;
    \item Число потоков определяется через \texttt{ppc::util::GetPPCNumThreads()};
    \item Создаётся \texttt{task\_arena} для управления параллельными задачами с заданным числом потоков.
  \end{itemize}

  \item \textbf{Параллельное вычисление частот (\texttt{ComputeFrequencyParallel}):}
  Используется \texttt{tbb::parallel\_reduce} для распределения работы по потокам. Каждый поток обрабатывает свой диапазон элементов, подсчитывая локальные частоты:
  \begin{lstlisting}
tbb::parallel_reduce(tbb::blocked_range<size_t>(0, a.size(), 10000), std::array<int, 256>{},
    [&](const tbb::blocked_range<size_t>& range, std::array<int, 256> local_count) {
        for (size_t i = range.begin(); i < range.end(); ++i) {
            unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
            if (shift == 24) key ^= 0x80;
            ++local_count[key];
        }
        return local_count;
    },
    [](const std::array<int, 256>& left, const std::array<int, 256>& right) {
        std::array<int, 256> result{};
        for (int i = 0; i < 256; ++i) {
            result[i] = left[i] + right[i];
        }
        return result;
    }, tbb::auto_partitioner());
  \end{lstlisting}
  Локальные счётчики объединяются в глобальный массив частот с помощью редукции, что минимизирует накладные расходы на синхронизацию.

  \item \textbf{Расчёт индексов (\texttt{ComputeIndices}):}
  Выполняется последовательно из-за зависимости данных: каждый индекс вычисляется как сумма предыдущих частот, формируя массив \texttt{index}.

  \item \textbf{Параллельное распределение элементов (\texttt{DistributeElementsParallel}):}
  Элементы распределяются в выходной массив с использованием чанков (блоков данных) для оптимизации работы потоков:
  \begin{itemize}
    \item Входной массив делится на чанки оптимального размера: $\text{optimal\_chunk\_size} = \max(1000, n / (T \cdot 4))$, где $T$ — число потоков;
    \item Каждый чанк обрабатывается отдельно с помощью \texttt{tbb::parallel\_for}, подсчитывая локальные частоты в структуре \texttt{ChunkInfo};
    \item Для каждого чанка вычисляются смещения (\texttt{chunk\_offsets}) на основе глобальных индексов и локальных частот;
    \item Элементы размещаются в выходной массив \texttt{b} с использованием локальных смещений, что исключает конфликты между потоками:
    \begin{lstlisting}
tbb::parallel_for(tbb::blocked_range<size_t>(0, num_chunks, 1),
    [&](const tbb::blocked_range<size_t>& chunk_range) {
        for (size_t chunk_id = chunk_range.begin(); chunk_id < chunk_range.end(); ++chunk_id) {
            auto local_offsets = chunk_offsets[chunk_id];
            const size_t start = chunks[chunk_id].start_idx;
            const size_t end = chunks[chunk_id].end_idx;
            for (size_t i = start; i < end; ++i) {
                unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
                if (shift == 24) key ^= 0x80;
                b[local_offsets[key]++] = a[i];
            }
        }
    }, tbb::auto_partitioner());
    \end{lstlisting}
  \end{itemize}

  \item \textbf{Цикл по разрядам:}
  В \texttt{RunImpl} выполняется цикл по сдвигам (0, 8, 16, 24 бита) внутри \texttt{task\_arena}. На каждой итерации вызываются \texttt{ComputeFrequencyParallel}, \texttt{ComputeIndices} и \texttt{DistributeElementsParallel}, после чего входной и выходной массивы меняются местами (\texttt{a.swap(b)}).

  \item \textbf{Постобработка (\texttt{PostProcessingImpl}):}
  Результат копируется из \texttt{output\_} в выходной буфер задачи. Для массивов размером более 1000 элементов используется \texttt{tbb::parallel\_for} с чанками размером 10000 для параллельного копирования, иначе применяется последовательное копирование с помощью \texttt{std::copy}.
\end{enumerate}

\textbf{Поддерживающие методы:}
\begin{itemize}
  \item \texttt{PreProcessingImpl()} — загружает входные данные из буфера задачи в вектор \texttt{input\_};
  \item \texttt{RunImpl()} — выполняет сортировку внутри \texttt{task\_arena} с ограничением числа потоков;
  \item \texttt{PostProcessingImpl()} — копирует отсортированный результат в выходной буфер с условным использованием параллелизма;
  \item \texttt{ValidationImpl()} — проверяет соответствие размеров входного и выходного массивов.
\end{itemize}

\textbf{Преимущества реализации TBB:}
\begin{itemize}
  \item Использование \texttt{tbb::parallel\_reduce} для вычисления частот минимизирует синхронизацию и повышает эффективность.
  \item Чанк-ориентированное распределение элементов устраняет конфликты между потоками, улучшая масштабируемость.
  \item Автоматическое управление задачами через \texttt{tbb::auto\_partitioner} и \texttt{task\_arena} упрощает настройку и оптимизацию.
  \item Условный параллелизм в постобработке адаптируется к размеру данных, снижая накладные расходы для малых массивов.
  \item Высокая производительность на многоядерных системах благодаря эффективному разделению работы.
\end{itemize}

\textbf{Итог:} Реализация Radix Sort с использованием Intel TBB обеспечивает высокую производительность и масштабируемость за счёт продуманного разделения данных на чанки, эффективной редукции частот и гибкого управления потоками через \texttt{task\_arena}.

\subsection{STL (std::thread)}

Реализация на основе стандартной библиотеки потоков \texttt{std::thread} требует явного управления потоками и синхронизацией. Параллелизм достигается как при вычислении частот, так и при распределении элементов, что позволяет ускорить выполнение алгоритма на многоядерных системах.

\textbf{Ключевая функция реализации:} \texttt{RadixSTL::RunImpl}.

\textbf{Основные этапы:}
\begin{enumerate}
  \item \textbf{Инициализация:}
  \begin{itemize}
    \item Определяется число потоков: \texttt{num\_threads = std::min(ppc::util::GetPPCNumThreads(), 3)}, что ограничивает использование до 3 потоков для предотвращения избыточной нагрузки.
    \item Задаётся порог для параллельного выполнения: \texttt{kParallelThreshold = 3000}. Если размер массива меньше этого значения, используется последовательная версия алгоритма.
  \end{itemize}

  \item \textbf{Параллельное вычисление частот:}
  Функция \texttt{ComputeFrequencyParallel} разбивает входной массив на части, и каждый поток вычисляет частоты для своей части. Локальные счётчики потоков затем объединяются в глобальный массив частот.

  \item \textbf{Расчёт индексов:}
  Функция \texttt{ComputeIndices} выполняется последовательно, так как вычисление индексов зависит от глобальных частот и имеет последовательную зависимость.

  \item \textbf{Параллельное распределение элементов:}
  В функции \texttt{DistributeElementsParallel} каждый поток вычисляет свои локальные счётчики и индексы на основе глобальных индексов. Затем потоки распределяют элементы своей части массива в выходной массив, избегая конфликтов благодаря использованию локальных смещений.

  \item \textbf{Цикл по разрядам:}
  В \texttt{RunImpl} для каждого сдвига (0, 8, 16, 24 бита) вычисляются частоты, индексы и распределяются элементы. Используются параллельные версии функций, если размер массива превышает порог и доступно более одного потока; в противном случае выполняется последовательная версия.
\end{enumerate}

\textbf{Вспомогательные методы:}
\begin{itemize}
  \item \texttt{PreProcessingImpl()} — загружает данные из буфера задачи во внутренний вектор \texttt{input\_};
  \item \texttt{RunImpl()} — запускает алгоритм сортировки, управляя выбором между параллельной и последовательной реализацией;
  \item \texttt{PostProcessingImpl()} — копирует отсортированные данные из \texttt{output\_} в выходной буфер;
  \item \texttt{ValidationImpl()} — проверяет соответствие размеров входного и выходного массивов.
\end{itemize}

\textbf{Преимущества реализации:}
\begin{itemize}
  \item Полный контроль над созданием, управлением и синхронизацией потоков, что позволяет глубже понять принципы параллельного программирования;
  \item Отсутствие внешних зависимостей, так как используется только стандартная библиотека C++, что упрощает интеграцию и переносимость кода.
\end{itemize}

\textbf{Вывод:} Реализация на основе \texttt{std::thread} подходит для небольших проектов или образовательных целей, предоставляя возможность изучения основ параллельного программирования. Однако для более крупных и сложных проектов предпочтительнее использовать более высокоуровневые средства, такие как OpenMP или Intel TBB, которые обеспечивают лучшую оптимизацию и масштабируемость.


\subsection{MPI + OpenMP}

Гибридная реализация с использованием MPI и OpenMP сочетает распределённые вычисления и многопоточность для эффективной сортировки больших массивов данных. Каждый MPI-процесс получает часть входного массива, сортирует её локально с использованием OpenMP, а затем результаты собираются и объединяются на корневом процессе с помощью древовидного подхода.

\textbf{Ключевая функция реализации:} \texttt{RadixALL::RunImpl}.

\textbf{Основные этапы реализации:}
\begin{enumerate}
  \item \textbf{Предобработка (\texttt{PreProcessingImpl}):}
  \begin{itemize}
    \item На процессе с рангом 0 входной массив загружается из буфера задачи в вектор \texttt{input\_}.
    \item Размер массива транслируется всем процессам с помощью \texttt{boost::mpi::broadcast}.
  \end{itemize}

  \item \textbf{Распределение данных (\texttt{DistributeData}):}
  \begin{itemize}
    \item На процессе 0 входной массив делится на части, где размер каждой части вычисляется как $\text{base\_chunk} = \lfloor n / \text{size} \rfloor$ с добавлением остатка для первых процессов.
    \item Данные распределяются между процессами: корневой процесс копирует свою часть локально, а остальные части отправляются другим процессам с помощью \texttt{world\_.send} и \texttt{world\_.recv}.
    \item Каждый процесс получает локальный массив \texttt{local\_data}.
  \end{itemize}

  \item \textbf{Локальная сортировка (\texttt{RadixSortLocal}):}
  \begin{itemize}
    \item Локальный массив разделяется на отрицательные и положительные числа в параллельном режиме с использованием OpenMP:
    \begin{lstlisting}
#pragma omp parallel
{
    std::vector<int> local_neg;
    std::vector<int> local_pos;
#pragma omp for nowait
    for (int i = 0; i < static_cast<int>(arr.size()); ++i) {
        if (arr[i] < 0) {
            local_neg.push_back(-arr[i]);
        } else {
            local_pos.push_back(arr[i]);
        }
    }
#pragma omp critical
    {
        negatives.insert(negatives.end(), local_neg.begin(), local_neg.end());
        positives.insert(positives.end(), local_pos.begin(), local_pos.end());
    }
}
    \end{lstlisting}
    \item Отрицательные и положительные числа сортируются параллельно с помощью \texttt{\#pragma omp parallel sections} в функции \texttt{RadixSortPositive}, которая использует поразрядную сортировку по цифрам (counting sort).
    \item Для отрицательных чисел после сортировки выполняется реверс и инверсия знака.
    \item Отсортированные части объединяются в локальный массив.
  \end{itemize}

  \item \textbf{Поразрядная сортировка (\texttt{RadixSortPositive}, \texttt{CountingSortByDigit}):}
  \begin{itemize}
    \item Максимальное значение в массиве находится параллельно с использованием OpenMP и критической секции для обновления \texttt{max\_val}.
    \item Для каждой позиции цифры (начиная с единиц) выполняется сортировка подсчётом:
    \begin{itemize}
      \item Каждый поток подсчитывает частоты цифр (0--9) в своём диапазоне, используя локальные счётчики \texttt{thread\_counts}.
      \item Локальные счётчики объединяются в глобальный массив \texttt{count}.
      \item Элементы распределяются в выходной массив на основе кумулятивных частот.
    \end{itemize}
  \end{itemize}

  \item \textbf{Сбор и слияние результатов (\texttt{GatherAndMerge}):}
  \begin{itemize}
    \item Используется древовидный подход для масштабируемого объединения отсортированных частей.
    \item На каждом шаге процесса с рангом, кратным $2 \cdot \text{step}$, принимают данные от процесса с рангом $\text{rank} + \text{step}$ и выполняют слияние с помощью \texttt{MergeTwoSorted}.
    \item Процессы, чей ранг кратен $\text{step}$, отправляют свои данные процессу с меньшим рангом.
    \item Итоговый отсортированный массив собирается на процессе 0.
  \end{itemize}

  \item \textbf{Постобработка (\texttt{PostProcessingImpl}):}
  \begin{itemize}
    \item На процессе 0 отсортированный массив \texttt{output\_} копируется в выходной буфер задачи с помощью \texttt{std::memcpy}.
  \end{itemize}
\end{enumerate}

\textbf{Функции реализации:}
\begin{itemize}
  \item \texttt{PreProcessingImpl()} — загрузка входного массива на процессе 0;
  \item \texttt{RunImpl()} — координация распределения данных, локальной сортировки и сбора результатов;
  \item \texttt{PostProcessingImpl()} — копирование результата в выходной буфер;
  \item \texttt{ValidationImpl()} — проверка соответствия размеров входного и выходного массивов на процессе 0;
  \item \texttt{DistributeData()} — распределение входного массива между MPI-процессами;
  \item \texttt{GatherAndMerge()} — сбор и слияние отсортированных частей с использованием древовидной редукции;
  \item \texttt{RadixSortLocal()} — локальная сортировка с Anchors негативных и положительных чисел с OpenMP;
  \item \texttt{RadixSortPositive()} — поразрядная сортировка положительных чисел;
  \item \texttt{CountingSortByDigit()} — сортировка подсчётом по текущей цифре;
  \item \texttt{MergeTwoSorted()} — слияние двух отсортированных массивов.
\end{itemize}

\textbf{Преимущества:}
\begin{itemize}
  \item Комбинация MPI и OpenMP обеспечивает масштабируемость на кластерах и эффективное использование многоядерных процессоров внутри узлов.
  \item Древовидное слияние результатов минимизирует коммуникационные задержки при большом числе процессов.
  \item Параллельная обработка отрицательных и положительных чисел с помощью OpenMP повышает производительность локальной сортировки.
  \item Гибкость подхода позволяет эффективно обрабатывать большие массивы данных в распределённой среде.
\end{itemize}

\textbf{Вывод:} Гибридная реализация MPI+OpenMP обеспечивает высокую производительность и масштабируемость за счёт распределения данных между процессами и параллельной обработки внутри каждого узла, что делает её подходящей для высокопроизводительных вычислений на кластерах.

\section{Результаты экспериментов}

\hspace*{1.25em}Для оценки производительности реализованных версий алгоритма поразрядной сортировки (Radix Sort) с простым слиянием были проведены замеры времени выполнения в конфигурациях с различным числом потоков и процессов. В таблице ниже представлены усреднённые значения времени выполнения (в секундах) для двух режимов тестирования — \texttt{PipelineRun} и \texttt{TaskRun}, а также рассчитано ускорение (\textit{Speedup}) относительно последовательной версии, определённое как отношение времени \texttt{TaskRun} последовательной реализации к времени \texttt{TaskRun} параллельной реализации.

\subsection{Таблица производительности}

\renewcommand{\arraystretch}{1.4}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Версия} & \textbf{Конфигурация} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Speedup} \\
\hline
\textbf{Последовательная} & — & 3.987 & 3.868 & 1.00 \\
\hline
\multirow{3}{*}{OpenMP} 
  & 2 потока & 3.710 & 1.682 & 2.30 \\
  & 3 потока & 3.196 & 1.561 & 2.48 \\
  & 4 потока & 3.044 & 1.542 & 2.51 \\
\hline
\multirow{3}{*}{TBB} 
  & 2 потока & 3.521 & 1.646 & 2.35 \\
  & 3 потока & 3.117 & 1.500 & 2.58 \\
  & 4 потока & 3.002 & 1.455 & 2.66 \\
\hline
\multirow{3}{*}{STL (std::thread)} 
  & 2 потока & 4.602 & 1.857 & 2.08 \\
  & 3 потока & 4.224 & 1.758 & 2.20 \\
  & 4 потока & 3.849 & 1.669 & 2.32 \\
\hline
\multirow{4}{*}{OMP + MPI} 
  & 2 потока + 2 процесса & 2.922 & 2.947 & 1.31 \\
  & 2 потока + 4 процесса & 2.748 & 2.713 & 1.43 \\
  & 4 потока + 2 процесса & 3.170 & 3.226 & 1.20 \\
  & 4 потока + 4 процесса & 3.011 & 3.176 & 1.22 \\
\hline
\end{tabular}
\caption{Производительность различных реализаций Radix Sort}
\label{tab:parallel_perf}
\end{table}

\subsection*{Анализ}
\hspace*{1.25em}На основе таблицы видно, что наибольшее ускорение достигается в реализации \textbf{TBB} с конфигурацией \textbf{4 потока}, где \textit{Speedup} составляет \textbf{2.66}. Реализация \textbf{OpenMP} также демонстрирует высокую эффективность, достигая \textit{Speedup} 2.51 при 4 потоках. Реализация \textbf{STL (std::thread)} показывает умеренное ускорение, с максимальным значением 2.32 при 4 потоках, что ниже, чем у TBB и OpenMP, вероятно, из-за более высоких накладных расходов на управление потоками.

\hspace*{1.25em}Гибридная реализация \textbf{MPI + OMP} демонстрирует наименьшее ускорение, с максимальным \textit{Speedup} 1.43 при конфигурации 2 потока + 4 процесса. Это может быть связано с дополнительными накладными расходами на коммуникацию между процессами в MPI, которые перевешивают выгоды от распределённой обработки в данном случае. Увеличение числа потоков и процессов в MPI + OMP не приводит к значительному улучшению производительности, что указывает на ограниченную масштабируемость этой реализации для текущего набора данных.

\hspace*{1.25em}Сравнение \texttt{PipelineRun} и \texttt{TaskRun} показывает, что \texttt{TaskRun} в параллельных реализациях существенно быстрее, что объясняется оптимизацией вычислений в основном этапе сортировки. Однако в \texttt{PipelineRun} время выполнения остаётся высоким, особенно для STL, что может быть связано с дополнительными накладными расходами на предобработку и постобработку данных.

\subsection*{Выводы}
\hspace*{1.25em}Реализация на основе \textbf{TBB} оказалась наиболее эффективной для локальной многопоточной обработки, обеспечивая максимальное ускорение благодаря автоматическому управлению задачами и минимальным накладным расходам. \textbf{OpenMP} также показала высокую производительность и является хорошим выбором для систем с общей памятью благодаря простоте интеграции. Реализация \textbf{STL (std::thread)} менее эффективна из-за необходимости явного управления потоками, но может быть полезна в образовательных целях или в проектах без внешних зависимостей.

\hspace*{1.25em}Гибридная реализация \textbf{MPI + OMP} не оправдала ожиданий в текущем эксперименте, вероятно, из-за высоких накладных расходов на коммуникацию между процессами. Для больших наборов данных и более мощных кластерных систем эта реализация может показать лучшие результаты, но в данном случае её эффективность ограничена.

\hspace*{1.25em}Таким образом, для локальной обработки больших массивов данных оптимальным выбором является \textbf{TBB} или \textbf{OpenMP}, в зависимости от требований к простоте реализации. Для распределённых систем рекомендуется дальнейшая оптимизация MPI + OMP, возможно, с использованием более крупных наборов данных или улучшенных стратегий распределения.

\section{Заключение}

\hspace*{1.25em}В данной работе была реализована и проанализирована модифицированная версия алгоритма поразрядной сортировки (Radix Sort) с добавлением этапа простого слияния отсортированных блоков. Базовая последовательная реализация была дополнена четырьмя параллельными версиями с использованием современных технологий: OpenMP, Intel TBB, стандартной библиотеки потоков C++ (STL) и гибридной модели MPI + OpenMP.

\hspace*{1.25em}Каждая технология была выбрана для изучения её эффективности и особенностей при реализации параллельных алгоритмов для обработки больших массивов целых чисел. Были решены задачи распределения данных между потоками и процессами, параллельного вычисления частот, распределения элементов и слияния отсортированных частей. Экспериментальные замеры позволили сравнить производительность всех реализаций по времени выполнения и ускорению.

\hspace*{1.25em}Наивысшее ускорение продемонстрировала реализация \textbf{TBB} в конфигурации с 4 потоками, достигнув \textit{Speedup} 2.66 относительно последовательной версии. Реализация \textbf{OpenMP} также показала высокую эффективность с \textit{Speedup} 2.51 при 4 потоках. Реализация \textbf{STL (std::thread)} обеспечила умеренное ускорение (до 2.32 при 4 потоках), что ниже TBB и OpenMP из-за более высоких накладных расходов на управление потоками. Гибридная реализация \textbf{MPI + OMP} показала наименьшую эффективность (максимальный \textit{Speedup} 1.43), вероятно, из-за значительных коммуникационных затрат в MPI, которые ограничивают масштабируемость при текущем размере данных.

\hspace*{1.25em}Таким образом, все поставленные цели работы были достигнуты:
\begin{itemize}
  \item Разработан и протестирован модифицированный алгоритм поразрядной сортировки с простым слиянием;
  \item Реализованы параллельные версии с использованием OpenMP, TBB, STL и MPI + OMP;
  \item Проведён экспериментальный анализ производительности всех реализаций;
  \item Выявлены преимущества и ограничения каждой технологии в контексте масштабируемости и удобства разработки.
\end{itemize}

\hspace*{1.25em}Результаты подчеркивают высокую эффективность локальных параллельных реализаций, особенно TBB и OpenMP, для задач сортировки на многоядерных системах. Гибридная модель MPI + OMP требует дальнейшей оптимизации, возможно, с использованием более крупных наборов данных или улучшенных стратегий распределения, чтобы реализовать её потенциал в распределённых системах. В перспективе возможно исследование адаптивных алгоритмов распределения нагрузки и оптимизации коммуникаций для повышения производительности гибридных реализаций.

\section{Список литературы}
\begin{enumerate}
    \item Cormen, T. H., Leiserson, C. E., Rivest, R. L., \& Stein, C. (2009). \textit{Introduction to Algorithms}. MIT Press.  
    \item Knuth, D. E. (1998). \textit{The Art of Computer Programming, Volume 3: Sorting and Searching}. Addison-Wesley.  
    \item Chapman, B., Jost, G., \& van der Pas, R. (2008). \textit{Using OpenMP: Portable Shared Memory Parallel Programming}. MIT Press.  
    \item Chandra, R., Menon, R., Dagum, L., Kohr, D., Maydan, D., \& McDonald, J. (2001). \textit{Parallel Programming in OpenMP}. Morgan Kaufmann.  
    \item Reinders, J. (2007). \textit{Intel Threading Building Blocks: Outfitting C++ for Multi-core Processor Parallelism}. O'Reilly Media.  
    \item Voss, M., Biswas, A. K., Asenjo, R., \& Reinders, J. (2016). \textit{Pro TBB: C++ Parallel Programming with Threading Building Blocks}. Apress.  
    \item Williams, A. (2012). \textit{C++ Concurrency in Action: Practical Multithreading}. Manning Publications.  
    \item Filipek, B. (2020). \textit{Mastering C++ Multithreading}. Packt Publishing.  
    \item Gropp, W., Lusk, E., \& Skjellum, A. (1999). \textit{Using MPI: Portable Parallel Programming with the Message-Passing Interface}. MIT Press.  
    \item Snir, M., Otto, S., Huss-Lederman, S., Walker, D., \& Dongarra, J. (1996). \textit{MPI: The Complete Reference}. MIT Press.  
    \item Quinn, M. J. (2003). \textit{Parallel Programming in C with MPI and OpenMP}. McGraw-Hill.  
    \item Quinn, M. J. (1994). \textit{Parallel Computing: Theory and Practice}. McGraw-Hill.  
    \item Mattson, T. G., Sanders, B. A., \& Massingill, B. L. (2004). \textit{Patterns for Parallel Programming}. Addison-Wesley.  
\end{enumerate}
\appendix
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}

В этом разделе приведены ключевые фрагменты реализации алгоритма поразрядной сортировки (Radix Sort) с простым слиянием для различных технологий распараллеливания.

\subsection*{OpenMP: функции ComputeFrequency, ComputeIndices и DistributeElements}
\begin{lstlisting}[language=C++]
std::array<int, 256> burykin_m_radix_omp::RadixOMP::ComputeFrequency(const std::vector<int>& a, const int shift) {
  std::array<int, 256> count = {};
  const int n = static_cast<int>(a.size());

#pragma omp parallel
  {
    std::array<int, 256> local_count = {};

#pragma omp for nowait schedule(static)
    for (int i = 0; i < n; ++i) {
      const int v = a[i];
      unsigned int key = ((static_cast<unsigned int>(v) >> shift) & 0xFFU);
      if (shift == 24) {
        key ^= 0x80;
      }
      ++local_count[key];
    }

    for (int bucket = 0; bucket < 256; ++bucket) {
      if (local_count[bucket] > 0) {
#pragma omp atomic
        count[bucket] += local_count[bucket];
      }
    }
  }

  return count;
}

std::array<int, 256> burykin_m_radix_omp::RadixOMP::ComputeIndices(const std::array<int, 256>& count) {
  std::array<int, 256> index = {0};
  for (int i = 1; i < 256; ++i) {
    index[i] = index[i - 1] + count[i - 1];
  }
  return index;
}

void burykin_m_radix_omp::RadixOMP::DistributeElements(const std::vector<int>& a, std::vector<int>& b,
                                                       std::array<int, 256> index, const int shift) {
  const int n = static_cast<int>(a.size());

  const int num_threads = omp_get_max_threads();
  std::vector<std::array<int, 256>> thread_indices(num_threads);

  for (int t = 0; t < num_threads; ++t) {
    thread_indices[t] = index;
  }

  std::vector<std::array<int, 256>> thread_counts(num_threads);
  for (auto& tc : thread_counts) {
    tc.fill(0);
  }

#pragma omp parallel
  {
    const int tid = omp_get_thread_num();
    const int chunk_size = (n + num_threads - 1) / num_threads;
    const int start = tid * chunk_size;
    const int end = std::min(start + chunk_size, n);

    for (int i = start; i < end; ++i) {
      const int v = a[i];
      unsigned int key = ((static_cast<unsigned int>(v) >> shift) & 0xFFU);
      if (shift == 24) {
        key ^= 0x80;
      }
      ++thread_counts[tid][key];
    }
  }

  for (int bucket = 0; bucket < 256; ++bucket) {
    int offset = index[bucket];
    for (int t = 0; t < num_threads; ++t) {
      thread_indices[t][bucket] = offset;
      offset += thread_counts[t][bucket];
    }
  }

#pragma omp parallel
  {
    const int tid = omp_get_thread_num();
    const int chunk_size = (n + num_threads - 1) / num_threads;
    const int start = tid * chunk_size;
    const int end = std::min(start + chunk_size, n);

    auto local_indices = thread_indices[tid];

    for (int i = start; i < end; ++i) {
      const int v = a[i];
      unsigned int key = ((static_cast<unsigned int>(v) >> shift) & 0xFFU);
      if (shift == 24) {
        key ^= 0x80;
      }

      b[local_indices[key]++] = v;
    }
  }
}
\end{lstlisting}

\subsection*{TBB: функции ComputeFrequencyParallel, ComputeIndices и DistributeElementsParallel}
\begin{lstlisting}[language=C++]
std::array<int, 256> burykin_m_radix_tbb::RadixTBB::ComputeFrequencyParallel(const std::vector<int>& a,
                                                                             const int shift) {
  return tbb::parallel_reduce(
      tbb::blocked_range<size_t>(0, a.size(), 10000), std::array<int, 256>{},
      [&](const tbb::blocked_range<size_t>& range, std::array<int, 256> local_count) {
        for (size_t i = range.begin(); i < range.end(); ++i) {
          unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
          if (shift == 24) {
            key ^= 0x80;
          }
          ++local_count[key];
        }
        return local_count;
      },
      [](const std::array<int, 256>& left, const std::array<int, 256>& right) {
        std::array<int, 256> result{};
        for (int i = 0; i < 256; ++i) {
          result[i] = left[i] + right[i];
        }
        return result;
      },
      tbb::auto_partitioner());
}

std::array<int, 256> burykin_m_radix_tbb::RadixTBB::ComputeIndices(const std::array<int, 256>& count) {
  std::array<int, 256> index = {0};
  for (int i = 1; i < 256; ++i) {
    index[i] = index[i - 1] + count[i - 1];
  }
  return index;
}

void burykin_m_radix_tbb::RadixTBB::DistributeElementsParallel(const std::vector<int>& a, std::vector<int>& b,
                                                               const std::array<int, 256>& index, const int shift) {
  const size_t num_threads = ppc::util::GetPPCNumThreads();
  const size_t n = a.size();

  const size_t optimal_chunk_size = std::max(size_t(1000), n / (num_threads * 4));
  const size_t num_chunks = (n + optimal_chunk_size - 1) / optimal_chunk_size;

  struct ChunkInfo {
    std::array<int, 256> counts{};
    size_t start_idx{};
    size_t end_idx{};
  };

  std::vector<ChunkInfo> chunks(num_chunks);

  tbb::parallel_for(
      tbb::blocked_range<size_t>(0, num_chunks, 1),
      [&](const tbb::blocked_range<size_t>& chunk_range) {
        for (size_t chunk_id = chunk_range.begin(); chunk_id < chunk_range.end(); ++chunk_id) {
          const size_t start = chunk_id * optimal_chunk_size;
          const size_t end = std::min(start + optimal_chunk_size, n);

          chunks[chunk_id].start_idx = start;
          chunks[chunk_id].end_idx = end;
          chunks[chunk_id].counts.fill(0);

          for (size_t i = start; i < end; ++i) {
            unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
            if (shift == 24) {
              key ^= 0x80;
            }
            ++chunks[chunk_id].counts[key];
          }
        }
      },
      tbb::auto_partitioner());

  std::vector<std::array<int, 256>> chunk_offsets(num_chunks);

  for (int bucket = 0; bucket < 256; ++bucket) {
    int offset = index[bucket];
    for (size_t chunk_id = 0; chunk_id < num_chunks; ++chunk_id) {
      chunk_offsets[chunk_id][bucket] = offset;
      offset += chunks[chunk_id].counts[bucket];
    }
  }

  tbb::parallel_for(
      tbb::blocked_range<size_t>(0, num_chunks, 1),
      [&](const tbb::blocked_range<size_t>& chunk_range) {
        for (size_t chunk_id = chunk_range.begin(); chunk_id < chunk_range.end(); ++chunk_id) {
          auto local_offsets = chunk_offsets[chunk_id];
          const size_t start = chunks[chunk_id].start_idx;
          const size_t end = chunks[chunk_id].end_idx;

          for (size_t i = start; i < end; ++i) {
            unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
            if (shift == 24) {
              key ^= 0x80;
            }
            b[local_offsets[key]++] = a[i];
          }
        }
      },
      tbb::auto_partitioner());
}
\end{lstlisting}

\subsection*{STL: функции ComputeFrequencyParallel, ComputeThreadCounts, ComputeThreadIndices и DistributeElementsParallel}
\begin{lstlisting}[language=C++]
std::array<int, 256> burykin_m_radix_stl::RadixSTL::ComputeFrequencyParallel(const std::vector<int>& a, const int shift,
                                                                             int num_threads) {
  if (a.empty()) {
    return std::array<int, 256>{};
  }

  std::vector<std::array<int, 256>> thread_counts(num_threads);
  std::vector<std::thread> threads(num_threads);

  const size_t chunk_size = a.size() / static_cast<size_t>(num_threads);

  for (int t = 0; t < num_threads; ++t) {
    threads[t] = std::thread([&, t]() {
      const size_t start = static_cast<size_t>(t) * chunk_size;
      const size_t end = (t == num_threads - 1) ? a.size() : (static_cast<size_t>(t) + 1) * chunk_size;

      for (size_t i = start; i < end; ++i) {
        unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
        if (shift == 24) {
          key ^= 0x80;
        }
        ++thread_counts[t][key];
      }
    });
  }

  for (auto& thread : threads) {
    thread.join();
  }

  std::array<int, 256> total_count = {};
  for (const auto& count : thread_counts) {
    for (int i = 0; i < 256; ++i) {
      total_count[i] += count[i];
    }
  }

  return total_count;
}

void burykin_m_radix_stl::RadixSTL::ComputeThreadCounts(const std::vector<int>& a,
                                                        std::vector<std::array<int, 256>>& thread_counts,
                                                        const int shift, int num_threads, size_t chunk_size) {
  std::vector<std::thread> freq_threads(num_threads);

  for (int t = 0; t < num_threads; ++t) {
    freq_threads[t] = std::thread([&, t]() {
      const size_t start = static_cast<size_t>(t) * chunk_size;
      const size_t end = (t == num_threads - 1) ? a.size() : (static_cast<size_t>(t) + 1) * chunk_size;

      for (size_t i = start; i < end; ++i) {
        unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
        if (shift == 24) {
          key ^= 0x80;
        }
        ++thread_counts[t][key];
      }
    });
  }

  for (auto& thread : freq_threads) {
    thread.join();
  }
}

void burykin_m_radix_stl::RadixSTL::ComputeThreadIndices(std::vector<std::array<int, 256>>& thread_indices,
                                                         const std::vector<std::array<int, 256>>& thread_counts,
                                                         const std::array<int, 256>& global_index, int num_threads) {
  for (int k = 0; k < 256; ++k) {
    int offset = global_index[k];
    for (int t = 0; t < num_threads; ++t) {
      thread_indices[t][k] = offset;
      offset += thread_counts[t][k];
    }
  }
}

void burykin_m_radix_stl::RadixSTL::DistributeElementsParallel(const std::vector<int>& a, std::vector<int>& b,
                                                               const std::array<int, 256>& global_index,
                                                               const int shift, int num_threads) {
  std::vector<std::array<int, 256>> thread_indices(num_threads);
  std::vector<std::array<int, 256>> thread_counts(num_threads);

  const size_t chunk_size = a.size() / static_cast<size_t>(num_threads);

  ComputeThreadCounts(a, thread_counts, shift, num_threads, chunk_size);

  ComputeThreadIndices(thread_indices, thread_counts, global_index, num_threads);

  std::vector<std::thread> dist_threads(num_threads);

  for (int t = 0; t < num_threads; ++t) {
    dist_threads[t] = std::thread([&, t]() {
      const size_t start = static_cast<size_t>(t) * chunk_size;
      const size_t end = (t == num_threads - 1) ? a.size() : (static_cast<size_t>(t) + 1) * chunk_size;

      for (size_t i = start; i < end; ++i) {
        unsigned int key = ((static_cast<unsigned int>(a[i]) >> shift) & 0xFFU);
        if (shift == 24) {
          key ^= 0x80;
        }
        b[thread_indices[t][key]++] = a[i];
      }
    });
  }

  for (auto& thread : dist_threads) {
    thread.join();
  }
}
\end{lstlisting}

\subsection*{OMP + MPI: функции RadixSortLocal, RadixSortPositive, CountingSortByDigit, DistributeData, GatherAndMerge, MergeTwoSorted, SplitBySign и MergeResults}
\begin{lstlisting}[language=C++]
void burykin_m_radix_all::RadixALL::RadixSortLocal(std::vector<int>& arr) {
  if (arr.empty()) {
    return;
  }

  // Separate negative and positive numbers in parallel
  std::vector<int> negatives;
  std::vector<int> positives;

#pragma omp parallel
  {
    std::vector<int> local_neg;
    std::vector<int> local_pos;

#pragma omp for nowait
    for (int i = 0; i < static_cast<int>(arr.size()); ++i) {
      if (arr[i] < 0) {
        local_neg.push_back(-arr[i]);  // Store as positive
      } else {
        local_pos.push_back(arr[i]);
      }
    }

#pragma omp critical
    {
      negatives.insert(negatives.end(), local_neg.begin(), local_neg.end());
      positives.insert(positives.end(), local_pos.begin(), local_pos.end());
    }
  }

// Sort both parts in parallel
#pragma omp parallel sections
  {
#pragma omp section
    {
      if (!negatives.empty()) {
        RadixSortPositive(negatives);
        // Reverse and negate for correct order
        std::reverse(negatives.begin(), negatives.end());
#pragma omp parallel for
        for (int i = 0; i < static_cast<int>(negatives.size()); ++i) {
          negatives[i] = -negatives[i];
        }
      }
    }

#pragma omp section
    {
      if (!positives.empty()) {
        RadixSortPositive(positives);
      }
    }
  }

  // Merge results
  arr.clear();
  arr.reserve(negatives.size() + positives.size());
  arr.insert(arr.end(), negatives.begin(), negatives.end());
  arr.insert(arr.end(), positives.begin(), positives.end());
}

void burykin_m_radix_all::RadixALL::RadixSortPositive(std::vector<int>& arr) {
  if (arr.empty()) {
    return;
  }

  // Find maximum value in parallel
  int max_val = 0;
#pragma omp parallel
  {
    int local_max = 0;
#pragma omp for
    for (int i = 0; i < static_cast<int>(arr.size()); ++i) {
      local_max = std::max(arr[i], local_max);
    }
#pragma omp critical
    { max_val = std::max(local_max, max_val); }
  }

  // Process each digit position
  for (int exp = 1; max_val / exp > 0; exp *= 10) {
    CountingSortByDigit(arr, exp);
  }
}

void burykin_m_radix_all::RadixALL::CountingSortByDigit(std::vector<int>& arr, int exp) {
  const int n = static_cast<int>(arr.size());
  std::vector<int> output(n);
  std::vector<int> count(10, 0);

  // Parallel counting with thread-local counters
  const int num_threads = omp_get_max_threads();
  std::vector<std::vector<int>> thread_counts(num_threads, std::vector<int>(10, 0));

#pragma omp parallel
  {
    const int tid = omp_get_thread_num();

#pragma omp for schedule(static)
    for (int i = 0; i < n; i++) {
      const int digit = (arr[i] / exp) % 10;
      thread_counts[tid][digit]++;
    }
  }

  // Merge thread counts
  for (int t = 0; t < num_threads; ++t) {
    for (int d = 0; d < 10; ++d) {
      count[d] += thread_counts[t][d];
    }
  }

  // Convert to cumulative counts
  std::partial_sum(count.begin(), count.end(), count.begin());

  // Build output array
  for (int i = n - 1; i >= 0; i--) {
    const int digit = (arr[i] / exp) % 10;
    output[--count[digit]] = arr[i];
  }

  arr = std::move(output);
}

std::vector<int> burykin_m_radix_all::RadixALL::DistributeData(const std::vector<int>& data, int rank, int size) {
  std::vector<int> local_data;
  std::vector<int> send_counts(size);
  std::vector<int> displs(size);

  if (rank == 0) {
    // Calculate optimal distribution
    const size_t total_size = data.size();
    const size_t base_chunk = total_size / size;
    const size_t remainder = total_size % size;

    size_t offset = 0;
    for (int i = 0; i < size; ++i) {
      send_counts[i] = static_cast<int>(base_chunk + (i < static_cast<int>(remainder) ? 1 : 0));
      displs[i] = static_cast<int>(offset);
      offset += send_counts[i];
    }
  }

  // Broadcast distribution info
  boost::mpi::broadcast(world_, send_counts, 0);

  // Resize local buffer
  local_data.resize(send_counts[rank]);

  // Scatter data efficiently
  if (rank == 0) {
    for (int i = 0; i < size; ++i) {
      if (i == 0) {
        std::copy(data.begin(), data.begin() + send_counts[0], local_data.begin());
      } else {
        std::vector<int> chunk(data.begin() + displs[i], data.begin() + displs[i] + send_counts[i]);
        world_.send(i, 0, chunk);
      }
    }
  } else {
    world_.recv(0, 0, local_data);
  }

  return local_data;
}

std::vector<int> burykin_m_radix_all::RadixALL::GatherAndMerge(const std::vector<int>& local_sorted, int rank,
                                                               int size) {
  std::vector<int> current_data = local_sorted;

  // Tree-based reduction for better scalability
  for (int step = 1; step < size; step *= 2) {
    if (rank % (2 * step) == 0) {
      // Receiver
      int sender = rank + step;
      if (sender < size) {
        std::vector<int> received_data;
        world_.recv(sender, 0, received_data);
        current_data = MergeTwoSorted(current_data, received_data);
      }
    } else if (rank % step == 0) {
      // Sender
      int receiver = rank - step;
      world_.send(receiver, 0, current_data);
      break;
    }
  }

  return current_data;
}

std::vector<int> burykin_m_radix_all::RadixALL::MergeTwoSorted(const std::vector<int>& left,
                                                               const std::vector<int>& right) {
  if (left.empty()) {
    return right;
  }
  if (right.empty()) {
    return left;
  }

  std::vector<int> result;
  result.reserve(left.size() + right.size());

  size_t i = 0;
  size_t j = 0;
  while (i < left.size() && j < right.size()) {
    if (left[i] <= right[j]) {
      result.push_back(left[i++]);
    } else {
      result.push_back(right[j++]);
    }
  }

  while (i < left.size()) {
    result.push_back(left[i++]);
  }
  while (j < right.size()) {
    result.push_back(right[j++]);
  }

  return result;
}

void burykin_m_radix_all::RadixALL::SplitBySign(const std::vector<int>& arr, std::vector<int>& negatives,
                                                std::vector<int>& positives) {
  negatives.clear();
  positives.clear();

  for (int num : arr) {
    if (num < 0) {
      negatives.push_back(-num);
    } else {
      positives.push_back(num);
    }
  }
}

void burykin_m_radix_all::RadixALL::MergeResults(std::vector<int>& result, const std::vector<int>& negatives,
                                                 const std::vector<int>& positives) {
  result.clear();
  result.reserve(negatives.size() + positives.size());
  result.insert(result.end(), negatives.begin(), negatives.end());
  result.insert(result.end(), positives.begin(), positives.end());
}
\end{lstlisting}
\end{document}